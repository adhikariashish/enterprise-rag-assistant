# =========================
# App
# =========================
APP_ENV=dev
LOG_LEVEL=INFO

# =========================
# Provider selection
# (should match ProvidersConfig literals)
# =========================
PROVIDERS__LLM=ollama
PROVIDERS__EMBEDDER=ollama
PROVIDERS__STORE=chroma

# =========================
# Ollama runtime wiring
# - Local run: http://localhost:11434
# - Docker-to-docker: http://ollama:11434  (service name in compose)
# =========================
OLLAMA_API_URL=http://localhost:11434

# Optional: if you support overriding model names at runtime
OLLAMA_LLM_MODEL=mistral:7b-instruct-q4_0
OLLAMA_EMBED_MODEL=nomic-embed-text

# Optional
OLLAMA_TIMEOUT_S=180

# =========================
# OpenAI (To switch providers=openai)
# =========================
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-5o-mini

# =========================
# Chroma
# =========================
CHROMA_PERSIST_DIR=storage/vectordb
CHROMA_COLLECTION=consultancy_kb
